## 1. Recap sobre Embeddings

### 1.1 Conceitos Fundamentais
Embeddings s√£o representa√ß√µes num√©ricas densas que capturam o significado sem√¢ntico de palavras, frases ou documentos em um espa√ßo vetorial de dimens√£o fixa. Eles resolvem limita√ß√µes das representa√ß√µes tradicionais como one-hot encoding.

```python
# Exemplo conceitual: representa√ß√£o de palavras como vetores
import numpy as np

# Representa√ß√£o tradicional (one-hot encoding)
vocabulario = ["gato", "cachorro", "animal", "felino"]
gato_onehot = [1, 0, 0, 0]
cachorro_onehot = [0, 1, 0, 0]

# Representa√ß√£o com embeddings (vetores densos)
gato_embedding = np.array([0.2, 0.8, 0.1, 0.9])
cachorro_embedding = np.array([0.3, 0.7, 0.2, 0.8])

print("=== COMPARA√á√ÉO DE REPRESENTA√á√ïES ===")
print(f"One-hot 'gato': {gato_onehot}")
print(f"Embedding 'gato': {gato_embedding}")
print(f"\nTamanho do vocabul√°rio: {len(vocabulario)}")
print(f"Dimens√µes one-hot: {len(gato_onehot)} (esparso: {gato_onehot.count(0)}/{len(gato_onehot)} zeros)")
print(f"Dimens√µes embedding: {len(gato_embedding)} (denso: todos valores n√£o-zero)")
```

**Principais diferen√ßas:**
- **One-hot encoding**: Vetores esparsos com apenas um 1 e muitos 0s
- **Embeddings**: Vetores densos com valores reais que capturam rela√ß√µes sem√¢nticas
- **Dimensionalidade**: One-hot cresce com o vocabul√°rio; embeddings t√™m tamanho fixo

### 1.2 Vantagens dos Embeddings

Os embeddings permitem que **palavras semanticamente similares tenham representa√ß√µes pr√≥ximas** no espa√ßo vetorial, algo imposs√≠vel com one-hot encoding.

```python
from sklearn.metrics.pairwise import cosine_similarity

# Demonstra√ß√£o de similaridade sem√¢ntica
embeddings = {
    "gato": np.array([0.2, 0.8, 0.1, 0.9]),
    "felino": np.array([0.25, 0.75, 0.15, 0.85]),
    "cachorro": np.array([0.3, 0.7, 0.2, 0.8]),
    "carro": np.array([0.9, 0.1, 0.8, 0.2])
}

# Calculando similaridade
sim_gato_felino = cosine_similarity([embeddings["gato"]], [embeddings["felino"]])[0][0]
sim_gato_carro = cosine_similarity([embeddings["gato"]], [embeddings["carro"]])[0][0]

print("=== AN√ÅLISE DE SIMILARIDADE SEM√ÇNTICA ===")
print(f"Similaridade gato ‚Üî felino: {sim_gato_felino:.3f}")
print(f"Similaridade gato ‚Üî cachorro: {sim_gato_cachorro:.3f}")
print(f"Similaridade gato ‚Üî carro: {sim_gato_carro:.3f}")

# Interpreta√ß√£o dos resultados
print("\n=== INTERPRETA√á√ÉO ===")
if sim_gato_felino > sim_gato_cachorro:
    print("‚úì 'Gato' √© mais similar a 'felino' (mesmo conceito)")
if sim_gato_cachorro > sim_gato_carro:
    print("‚úì 'Gato' √© mais similar a 'cachorro' (ambos animais) que a 'carro'")

# Demonstra√ß√£o com one-hot (para compara√ß√£o)
print("\n=== COMPARA√á√ÉO: ONE-HOT vs EMBEDDINGS ===")
onehot_gato = [1, 0, 0, 0]
onehot_felino = [0, 0, 0, 1]  # Posi√ß√£o diferente no vocabul√°rio
onehot_similarity = cosine_similarity([onehot_gato], [onehot_felino])[0][0]
print(f"Similaridade one-hot gato ‚Üî felino: {onehot_similarity:.3f}")
print("‚ùå One-hot n√£o captura que 'gato' e 'felino' s√£o relacionados!")
```

### 1.3 Propriedades Matem√°ticas dos Embeddings

```python
# Demonstra√ß√£o de propriedades matem√°ticas
print("=== PROPRIEDADES MATEM√ÅTICAS ===")

def calculate_vector_properties(name, vector):
    norm = np.linalg.norm(vector)
    print(f"{name}:")
    print(f"  Vetor: {vector}")
    print(f"  Norma (magnitude): {norm:.3f}")
    print(f"  Normalizado: {vector/norm}")
    print()

for name, embedding in embeddings.items():
    calculate_vector_properties(name, embedding)

# Opera√ß√µes vetoriais
print("=== OPERA√á√ïES VETORIAIS ===")
# Analogia: gato est√° para felino assim como cachorro est√° para...?
analogy_vector = embeddings["felino"] - embeddings["gato"] + embeddings["cachorro"]
print(f"Vetor analogia (felino - gato + cachorro): {analogy_vector}")

# Encontrar palavra mais pr√≥xima da analogia
similarities_analogy = {}
for word, vec in embeddings.items():
    if word != "cachorro":  # Excluir a palavra de entrada
        sim = cosine_similarity([analogy_vector], [vec])[0][0]
        similarities_analogy[word] = sim

closest_word = max(similarities_analogy, key=similarities_analogy.get)
print(f"Palavra mais pr√≥xima da analogia: {closest_word} (similaridade: {similarities_analogy[closest_word]:.3f})")
```

### 1.4 Vantagens dos Embeddings - Resumo

#### üéØ **Principais Vantagens dos Embeddings**

1. **üéØ Capturam similaridade sem√¢ntica**
   - Palavras com significados similares t√™m representa√ß√µes pr√≥ximas no espa√ßo vetorial
   - Exemplo: "gato" e "felino" ter√£o alta similaridade cosseno

2. **üìè Dimensionalidade fixa**
   - Independente do tamanho do vocabul√°rio
   - One-hot: cresce com vocabul√°rio | Embeddings: tamanho fixo (ex: 300D)

3. **üî¢ Representa√ß√£o densa**
   - Todos os valores s√£o significativos (n√£o h√° zeros desnecess√°rios)
   - Uso eficiente do espa√ßo de mem√≥ria

4. **üßÆ Opera√ß√µes matem√°ticas significativas**
   - Analogias: "rei" - "homem" + "mulher" ‚âà "rainha"
   - Aritm√©tica vetorial com significado sem√¢ntico

5. **üöÄ Efici√™ncia computacional**
   - Processamento mais r√°pido em modelos de Machine Learning
   - Menor dimensionalidade que one-hot para vocabul√°rios grandes

6. **üîÑ Transferibilidade entre tarefas**
   - Embeddings pr√©-treinados podem ser reutilizados
   - Word2Vec, GloVe, FastText servem para m√∫ltiplas aplica√ß√µes

#### üõ†Ô∏è **Aplica√ß√µes Pr√°ticas**

- **Sistemas de recomenda√ß√£o**: Encontrar produtos/conte√∫dos similares
- **Busca sem√¢ntica**: Buscar por significado, n√£o apenas palavras-chave
- **Tradu√ß√£o autom√°tica**: Mapear conceitos entre idiomas
- **An√°lise de sentimentos**: Capturar nuances emocionais
- **Classifica√ß√£o de textos**: Categoriza√ß√£o autom√°tica de documentos
- **Chatbots e assistentes virtuais**: Compreens√£o de inten√ß√µes do usu√°rio

#### üîë **Conceitos-chave para Fixa√ß√£o**

- **Similaridade Cosseno**: Mede o √¢ngulo entre vetores (0 = ortogonais, 1 = id√™nticos)
- **Espa√ßo Vetorial**: Embeddings vivem em um espa√ßo onde dist√¢ncia = similaridade sem√¢ntica
- **Densidade**: Cada dimens√£o contribui para o significado (vs. one-hot com muitos zeros)
- **Aprendizado**: Embeddings s√£o aprendidos automaticamente a partir de grandes corpus de texto
