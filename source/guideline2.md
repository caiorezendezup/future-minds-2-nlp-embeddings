## 2. Normaliza√ß√£o e Dimensionalidade dos Embeddings

### 2.1 Normaliza√ß√£o de Vetores

**Por que normalizar embeddings?**
A normaliza√ß√£o √© crucial em embeddings porque:
- **Elimina bias de magnitude**: Vetores com valores absolutos maiores n√£o dominam o c√°lculo de similaridade
- **Melhora performance**: Algoritmos convergem mais rapidamente
- **Padroniza escalas**: Diferentes features ficam na mesma escala
- **Facilita compara√ß√µes**: Similaridade cosseno se torna mais interpret√°vel

```python
import numpy as np
from sklearn.preprocessing import normalize

class EmbeddingNormalizer:
    @staticmethod
    def l2_normalize(embedding):
        """
        Normaliza√ß√£o L2 (norma euclidiana)
        
        F√≥rmula: x_norm = x / ||x||‚ÇÇ
        
        - Transforma o vetor para ter norma (magnitude) = 1
        - Preserva a dire√ß√£o do vetor original
        - Ideal para c√°lculo de similaridade cosseno
        - Mais comum em embeddings de texto
        """
        return embedding / np.linalg.norm(embedding)
    
    @staticmethod
    def min_max_normalize(embedding):
        """
        Normaliza√ß√£o Min-Max
        
        F√≥rmula: x_norm = (x - min(x)) / (max(x) - min(x))
        
        - Escala valores para o intervalo [0, 1]
        - Preserva a distribui√ß√£o original dos dados
        - Sens√≠vel a outliers
        - √ötil quando voc√™ conhece os limites dos dados
        """
        return (embedding - embedding.min()) / (embedding.max() - embedding.min())
    
    @staticmethod
    def z_score_normalize(embedding):
        """
        Normaliza√ß√£o Z-Score (Standardiza√ß√£o)
        
        F√≥rmula: x_norm = (x - Œº) / œÉ
        
        - Centraliza dados em m√©dia = 0, desvio padr√£o = 1
        - Assume distribui√ß√£o normal dos dados
        - Robusta a outliers moderados
        - Preserva a forma da distribui√ß√£o original
        """
        return (embedding - embedding.mean()) / embedding.std()

# Exemplo pr√°tico com interpreta√ß√£o educacional
embedding_raw = np.array([1.5, -0.8, 2.3, -1.2, 0.9])
normalizer = EmbeddingNormalizer()

embedding_l2 = normalizer.l2_normalize(embedding_raw)
embedding_minmax = normalizer.min_max_normalize(embedding_raw)
embedding_zscore = normalizer.z_score_normalize(embedding_raw)

print("=== COMPARA√á√ÉO DE T√âCNICAS DE NORMALIZA√á√ÉO ===")
print(f"Original: {embedding_raw}")
print(f"Norma original: {np.linalg.norm(embedding_raw):.3f}")
print()

print(f"L2 Normalizado: {embedding_l2}")
print(f"Nova norma: {np.linalg.norm(embedding_l2):.3f} (sempre = 1.0)")
print()

print(f"Min-Max Normalizado: {embedding_minmax}")
print(f"Min: {embedding_minmax.min():.3f}, Max: {embedding_minmax.max():.3f}")
print()

print(f"Z-Score Normalizado: {embedding_zscore}")
print(f"M√©dia: {embedding_zscore.mean():.3f}, Std: {embedding_zscore.std():.3f}")

# Demonstra√ß√£o pr√°tica: impacto na similaridade cosseno
print("\n=== IMPACTO NA SIMILARIDADE COSSENO ===")
embedding_a = np.array([1.0, 2.0, 3.0])
embedding_b = np.array([10.0, 20.0, 30.0])  # Mesmo padr√£o, escala diferente

from sklearn.metrics.pairwise import cosine_similarity

# Sem normaliza√ß√£o
sim_original = cosine_similarity([embedding_a], [embedding_b])[0][0]
print(f"Similaridade sem normaliza√ß√£o: {sim_original:.3f}")

# Com normaliza√ß√£o L2
a_norm = normalizer.l2_normalize(embedding_a)
b_norm = normalizer.l2_normalize(embedding_b)
sim_normalized = cosine_similarity([a_norm], [b_norm])[0][0]
print(f"Similaridade com normaliza√ß√£o L2: {sim_normalized:.3f}")
print("‚úì Vetores com mesmo padr√£o t√™m similaridade = 1.0 ap√≥s normaliza√ß√£o L2")
```

### 2.2 Redu√ß√£o de Dimensionalidade

**Por que reduzir dimensionalidade?**
- **Visualiza√ß√£o**: Imposs√≠vel visualizar espa√ßos com >3 dimens√µes
- **Performance**: Menos dimens√µes = processamento mais r√°pido
- **Armazenamento**: Reduz uso de mem√≥ria
- **Curse of dimensionality**: Evita problemas em espa√ßos de alta dimens√£o
- **Noise reduction**: Remove dimens√µes menos informativas

```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

class DimensionalityReducer:
    def __init__(self):
        self.pca = None
        self.tsne = None
    
    def reduce_with_pca(self, embeddings, n_components=2):
        """
        Redu√ß√£o usando PCA (Principal Component Analysis)
        
        Caracter√≠sticas:
        - Linear: Encontra combina√ß√µes lineares das features originais
        - Preserva vari√¢ncia: Mant√©m a maior vari√¢ncia poss√≠vel
        - Determin√≠stico: Sempre produz o mesmo resultado
        - R√°pido: Computacionalmente eficiente
        - Interpret√°vel: Componentes t√™m significado matem√°tico claro
        
        Quando usar:
        - Dados com correla√ß√µes lineares
        - Quando voc√™ precisa de resultados reproduz√≠veis
        - Para an√°lise explorat√≥ria inicial
        """
        self.pca = PCA(n_components=n_components)
        reduced = self.pca.fit_transform(embeddings)
        
        # Informa√ß√µes educacionais
        explained_variance = self.pca.explained_variance_ratio_
        print(f"PCA - Vari√¢ncia explicada por componente: {explained_variance}")
        print(f"Vari√¢ncia total preservada: {sum(explained_variance):.3f}")
        
        return reduced
    
    def reduce_with_tsne(self, embeddings, n_components=2):
        """
        Redu√ß√£o usando t-SNE (t-Distributed Stochastic Neighbor Embedding)
        
        Caracter√≠sticas:
        - N√£o-linear: Captura rela√ß√µes complexas
        - Preserva vizinhan√ßa local: Pontos pr√≥ximos ficam pr√≥ximos
        - Estoc√°stico: Resultados podem variar entre execu√ß√µes
        - Lento: Computacionalmente intensivo
        - Excelente para visualiza√ß√£o de clusters
        
        Quando usar:
        - Visualiza√ß√£o de dados complexos
        - Identifica√ß√£o de clusters
        - Quando rela√ß√µes s√£o n√£o-lineares
        
        Cuidados:
        - Dist√¢ncias globais n√£o s√£o preservadas
        - N√£o use para an√°lise quantitativa de dist√¢ncias
        """
        self.tsne = TSNE(n_components=n_components, random_state=42, 
                        perplexity=min(30, len(embeddings)-1))
        return self.tsne.fit_transform(embeddings)
    
    def plot_embeddings(self, embeddings_2d, labels=None, title="Visualiza√ß√£o de Embeddings"):
        """
        Visualiza√ß√£o dos embeddings reduzidos
        
        Par√¢metros:
        - embeddings_2d: Array com embeddings reduzidos para 2D
        - labels: Labels opcionais para anotar pontos
        - title: T√≠tulo do gr√°fico
        """
        plt.figure(figsize=(12, 8))
        
        # Colorir pontos se houver labels repetidos (clusters)
        if labels:
            unique_labels = list(set(labels))
            colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))
            
            for i, label in enumerate(unique_labels):
                mask = [l == label for l in labels]
                plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], 
                          c=[colors[i]], label=label, alpha=0.7, s=100)
        else:
            plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                       alpha=0.7, s=100, c='blue')
        
        # Anotar pontos
        if labels:
            for i, label in enumerate(labels):
                plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]),
                           xytext=(5, 5), textcoords='offset points',
                           fontsize=9, alpha=0.8)
        
        plt.title(title, fontsize=14, fontweight='bold')
        plt.xlabel("Dimens√£o 1", fontsize=12)
        plt.ylabel("Dimens√£o 2", fontsize=12)
        plt.grid(True, alpha=0.3)
        
        if labels and len(set(labels)) > 1:
            plt.legend()
        
        plt.tight_layout()
        plt.show()

# Exemplo pr√°tico educacional
def exemplo_reducao_dimensionalidade():
    """Exemplo completo comparando PCA vs t-SNE"""
    
    # Criar dados de exemplo: embeddings de palavras relacionadas
    np.random.seed(42)
    
    # CORRE√á√ÉO: Criar centros com 50 dimens√µes para cada categoria
    # Apenas as primeiras 3 dimens√µes s√£o diferentes, o resto √© zero
    centro_animais = np.zeros(50)
    centro_animais[:3] = [2, 1, 0]
    
    centro_frutas = np.zeros(50)
    centro_frutas[:3] = [-1, 2, 1]
    
    centro_cores = np.zeros(50)
    centro_cores[:3] = [0, -2, 2]s
    
    # Gerar embeddings com ru√≠do em torno dos centros
    animais = np.random.normal(centro_animais, 0.5, (10, 50))
    frutas = np.random.normal(centro_frutas, 0.5, (10, 50))
    cores = np.random.normal(centro_cores, 0.5, (10, 50))
    
    # Combinar dados
    embeddings = np.vstack([animais, frutas, cores])
    labels = (['animal'] * 10 + ['fruta'] * 10 + ['cor'] * 10)
    
    print(f"Dados originais: {embeddings.shape[0]} amostras, {embeddings.shape[1]} dimens√µes")
    
    # Inicializar redutor
    reducer = DimensionalityReducer()
    
    # Comparar PCA vs t-SNE
    print("\n=== REDU√á√ÉO COM PCA ===")
    embeddings_pca = reducer.reduce_with_pca(embeddings, n_components=2)
    
    print("\n=== REDU√á√ÉO COM t-SNE ===")
    embeddings_tsne = reducer.reduce_with_tsne(embeddings, n_components=2)
    
    # Visualizar resultados
    reducer.plot_embeddings(embeddings_pca, labels, "PCA - Redu√ß√£o Linear")
    reducer.plot_embeddings(embeddings_tsne, labels, "t-SNE - Redu√ß√£o N√£o-Linear")
    
    # An√°lise comparativa
    print("\n=== COMPARA√á√ÉO PCA vs t-SNE ===")
    print("PCA:")
    print("‚úì Preserva vari√¢ncia global")
    print("‚úì Resultados reproduz√≠veis")
    print("‚úì R√°pido para grandes datasets")
    print("‚úó Assume rela√ß√µes lineares")
    
    print("\nt-SNE:")
    print("‚úì Excelente para visualizar clusters")
    print("‚úì Captura rela√ß√µes n√£o-lineares")
    print("‚úì Preserva estrutura local")
    print("‚úó Dist√¢ncias globais n√£o s√£o confi√°veis")
    print("‚úó Lento para grandes datasets")

# Executar exemplo
exemplo_reducao_dimensionalidade()
```

### üéì **Pontos-chave:**

1. **Normaliza√ß√£o √© fundamental**: Sempre normalize embeddings antes de calcular similaridades
2. **L2 √© padr√£o**: Para embeddings de texto, normaliza√ß√£o L2 √© quase sempre a melhor escolha
3. **PCA vs t-SNE**: PCA para an√°lise quantitativa, t-SNE para visualiza√ß√£o de clusters
4. **Interpreta√ß√£o cuidadosa**: t-SNE pode criar clusters visuais que n√£o existem nos dados originais
5. **Experimenta√ß√£o**: Teste diferentes t√©cnicas para encontrar a melhor para seus dados
