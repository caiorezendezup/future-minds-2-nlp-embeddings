## 2. Normaliza√ß√£o e Dimensionalidade dos Embeddings

### 2.1 Normaliza√ß√£o de Vetores

**Por que normalizar embeddings?**
A normaliza√ß√£o √© crucial em embeddings porque:
- **Elimina bias de magnitude**: Vetores com valores absolutos maiores n√£o dominam o c√°lculo de similaridade
- **Melhora performance**: Algoritmos convergem mais rapidamente
- **Padroniza escalas**: Diferentes features ficam na mesma escala
- **Facilita compara√ß√µes**: Similaridade cosseno se torna mais interpret√°vel

```python
import numpy as np
from sklearn.preprocessing import normalize

class EmbeddingNormalizer:
    @staticmethod
    def l2_normalize(embedding):
        """
        Normaliza√ß√£o L2 (norma euclidiana)
        
        F√≥rmula: x_norm = x / ||x||‚ÇÇ
        
        - Transforma o vetor para ter norma (magnitude) = 1
        - Preserva a dire√ß√£o do vetor original
        - Ideal para c√°lculo de similaridade cosseno
        - Mais comum em embeddings de texto
        """
        return embedding / np.linalg.norm(embedding)
    
    @staticmethod
    def min_max_normalize(embedding):
        """
        Normaliza√ß√£o Min-Max
        
        F√≥rmula: x_norm = (x - min(x)) / (max(x) - min(x))
        
        - Escala valores para o intervalo [0, 1]
        - Preserva a distribui√ß√£o original dos dados
        - Sens√≠vel a outliers
        - √ötil quando voc√™ conhece os limites dos dados
        """
        return (embedding - embedding.min()) / (embedding.max() - embedding.min())
    
    @staticmethod
    def z_score_normalize(embedding):
        """
        Normaliza√ß√£o Z-Score (Standardiza√ß√£o)
        
        F√≥rmula: x_norm = (x - Œº) / œÉ
        
        - Centraliza dados em m√©dia = 0, desvio padr√£o = 1
        - Assume distribui√ß√£o normal dos dados
        - Robusta a outliers moderados
        - Preserva a forma da distribui√ß√£o original
        """
        return (embedding - embedding.mean()) / embedding.std()

# Exemplo pr√°tico com interpreta√ß√£o educacional
embedding_raw = np.array([1.5, -0.8, 2.3, -1.2, 0.9])
normalizer = EmbeddingNormalizer()

embedding_l2 = normalizer.l2_normalize(embedding_raw)
embedding_minmax = normalizer.min_max_normalize(embedding_raw)
embedding_zscore = normalizer.z_score_normalize(embedding_raw)

print("=== COMPARA√á√ÉO DE T√âCNICAS DE NORMALIZA√á√ÉO ===")
print(f"Original: {embedding_raw}")
print(f"Norma original: {np.linalg.norm(embedding_raw):.3f}")
print()

print(f"L2 Normalizado: {embedding_l2}")
print(f"Nova norma: {np.linalg.norm(embedding_l2):.3f} (sempre = 1.0)")
print()

print(f"Min-Max Normalizado: {embedding_minmax}")
print(f"Min: {embedding_minmax.min():.3f}, Max: {embedding_minmax.max():.3f}")
print()

print(f"Z-Score Normalizado: {embedding_zscore}")
print(f"M√©dia: {embedding_zscore.mean():.3f}, Std: {embedding_zscore.std():.3f}")

# Demonstra√ß√£o pr√°tica: impacto na similaridade cosseno
print("\n=== IMPACTO NA SIMILARIDADE COSSENO ===")
embedding_a = np.array([1.0, 2.0, 3.0])
embedding_b = np.array([10.0, 20.0, 30.0])  # Mesmo padr√£o, escala diferente

from sklearn.metrics.pairwise import cosine_similarity

# Sem normaliza√ß√£o
sim_original = cosine_similarity([embedding_a], [embedding_b])[0][0]
print(f"Similaridade sem normaliza√ß√£o: {sim_original:.3f}")

# Com normaliza√ß√£o L2
a_norm = normalizer.l2_normalize(embedding_a)
b_norm = normalizer.l2_normalize(embedding_b)
sim_normalized = cosine_similarity([a_norm], [b_norm])[0][0]
print(f"Similaridade com normaliza√ß√£o L2: {sim_normalized:.3f}")
print("‚úì Vetores com mesmo padr√£o t√™m similaridade = 1.0 ap√≥s normaliza√ß√£o L2")
```

### 2.2 Redu√ß√£o de Dimensionalidade

**Por que reduzir dimensionalidade?**
- **Visualiza√ß√£o**: Imposs√≠vel visualizar espa√ßos com >3 dimens√µes
- **Performance**: Menos dimens√µes = processamento mais r√°pido
- **Armazenamento**: Reduz uso de mem√≥ria
- **Curse of dimensionality**: Evita problemas em espa√ßos de alta dimens√£o
- **Noise reduction**: Remove dimens√µes menos informativas

```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

class DimensionalityReducer:
    def __init__(self):
        self.pca = None
        self.tsne = None
    
    def reduce_with_pca(self, embeddings, n_components=2):
        """
        Redu√ß√£o usando PCA (Principal Component Analysis)
        
        Caracter√≠sticas:
        - Linear: Encontra combina√ß√µes lineares das features originais
        - Preserva vari√¢ncia: Mant√©m a maior vari√¢ncia poss√≠vel
        - Determin√≠stico: Sempre produz o mesmo resultado
        - R√°pido: Computacionalmente eficiente
        - Interpret√°vel: Componentes t√™m significado matem√°tico claro
        
        Quando usar:
        - Dados com correla√ß√µes lineares
        - Quando voc√™ precisa de resultados reproduz√≠veis
        - Para an√°lise explorat√≥ria inicial
        - Quando os clusters s√£o bem separados linearmente
        """
        self.pca = PCA(n_components=n_components)
        reduced = self.pca.fit_transform(embeddings)
        
        # Informa√ß√µes educacionais
        explained_variance = self.pca.explained_variance_ratio_
        print(f"PCA - Vari√¢ncia explicada por componente: {explained_variance}")
        print(f"Vari√¢ncia total preservada: {sum(explained_variance):.3f}")
        
        return reduced
    
    def reduce_with_tsne(self, embeddings, n_components=2, perplexity=None):
        """
        Redu√ß√£o usando t-SNE (t-Distributed Stochastic Neighbor Embedding)
        
        Caracter√≠sticas:
        - N√£o-linear: Captura rela√ß√µes complexas
        - Preserva vizinhan√ßa local: Pontos pr√≥ximos ficam pr√≥ximos
        - Estoc√°stico: Resultados podem variar entre execu√ß√µes
        - Lento: Computacionalmente intensivo
        - Excelente para visualiza√ß√£o de clusters complexos
        
        Quando usar:
        - Visualiza√ß√£o de dados complexos e n√£o-lineares
        - Identifica√ß√£o de clusters em dados de alta dimens√£o
        - Quando rela√ß√µes s√£o n√£o-lineares
        - Dados com muitas amostras (>50)
        
        Cuidados:
        - Dist√¢ncias globais n√£o s√£o preservadas
        - N√£o use para an√°lise quantitativa de dist√¢ncias
        - Ajuste perplexity baseado no tamanho dos dados
        """
        # Ajustar perplexity automaticamente se n√£o fornecida
        if perplexity is None:
            # Regra pr√°tica: perplexity entre 5 e 50, ideal ~30
            # Para datasets pequenos, usar valor menor
            n_samples = len(embeddings)
            if n_samples < 30:
                perplexity = max(3, n_samples // 3)
            else:
                perplexity = min(30, n_samples - 1)
        
        print(f"t-SNE usando perplexity: {perplexity}")
        
        self.tsne = TSNE(
            n_components=n_components, 
            random_state=42,
            perplexity=perplexity,
            n_iter=1000,  # Mais itera√ß√µes para converg√™ncia
            learning_rate='auto'
        )
        return self.tsne.fit_transform(embeddings)
    
    def plot_embeddings(self, embeddings_2d, labels=None, title="Visualiza√ß√£o de Embeddings"):
        """
        Visualiza√ß√£o dos embeddings reduzidos com melhorias
        
        Par√¢metros:
        - embeddings_2d: Array com embeddings reduzidos para 2D
        - labels: Labels opcionais para anotar pontos
        - title: T√≠tulo do gr√°fico
        """
        plt.figure(figsize=(12, 8))
        
        # Colorir pontos se houver labels repetidos (clusters)
        if labels:
            unique_labels = list(set(labels))
            colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))
            
            for i, label in enumerate(unique_labels):
                mask = np.array([l == label for l in labels])
                plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], 
                          c=[colors[i]], label=label, alpha=0.8, s=120, edgecolors='black', linewidth=0.5)
        else:
            plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                       alpha=0.7, s=100, c='blue')
        
        # Anotar pontos com melhor posicionamento
        if labels:
            for i, label in enumerate(labels):
                plt.annotate(f"{label}_{i}", (embeddings_2d[i, 0], embeddings_2d[i, 1]),
                           xytext=(5, 5), textcoords='offset points',
                           fontsize=8, alpha=0.7, 
                           bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.7))
        
        plt.title(title, fontsize=14, fontweight='bold')
        plt.xlabel("Dimens√£o 1", fontsize=12)
        plt.ylabel("Dimens√£o 2", fontsize=12)
        plt.grid(True, alpha=0.3)
        
        if labels and len(set(labels)) > 1:
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        plt.tight_layout()
        plt.show()

# Exemplo pr√°tico educacional CORRIGIDO
def exemplo_reducao_dimensionalidade():
    """
    Exemplo completo comparando PCA vs t-SNE com dados mais realistas
    
    üéØ Objetivos educacionais:
    1. Mostrar quando PCA funciona melhor (dados lineares)
    2. Mostrar quando t-SNE funciona melhor (dados n√£o-lineares)
    3. Demonstrar a import√¢ncia dos par√¢metros
    """
    
    print("=" * 70)
    print("üéì DEMONSTRA√á√ÉO: PCA vs t-SNE - Compara√ß√£o Educacional")
    print("=" * 70)
    
    # Criar DOIS conjuntos de dados para compara√ß√£o
    np.random.seed(42)
    
    # === DATASET 1: CLUSTERS LINEARES (PCA deve funcionar melhor) ===
    print("\nüìä DATASET 1: CLUSTERS LINEARES")
    print("-" * 40)
    
    # Criar centros bem separados em alta dimens√£o
    n_dims = 50
    centro_animais = np.zeros(n_dims)
    centro_animais[:10] = np.random.normal(3, 0.5, 10)  # Varia√ß√£o em m√∫ltiplas dimens√µes
    
    centro_frutas = np.zeros(n_dims)
    centro_frutas[10:20] = np.random.normal(-3, 0.5, 10)
    
    centro_cores = np.zeros(n_dims)
    centro_cores[20:30] = np.random.normal(0, 0.5, 10)
    
    # Gerar amostras com mais variabilidade
    n_samples = 15  # Mais amostras por cluster
    animais_linear = np.random.normal(centro_animais, 0.8, (n_samples, n_dims))
    frutas_linear = np.random.normal(centro_frutas, 0.8, (n_samples, n_dims))
    cores_linear = np.random.normal(centro_cores, 0.8, (n_samples, n_dims))
    
    embeddings_linear = np.vstack([animais_linear, frutas_linear, cores_linear])
    labels_linear = (['animal'] * n_samples + ['fruta'] * n_samples + ['cor'] * n_samples)
    
    print(f"Dados lineares: {embeddings_linear.shape[0]} amostras, {embeddings_linear.shape[1]} dimens√µes")
    
    # === DATASET 2: CLUSTERS N√ÉO-LINEARES (t-SNE deve funcionar melhor) ===
    print("\nüìä DATASET 2: CLUSTERS N√ÉO-LINEARES")
    print("-" * 40)
    
    # Criar estruturas n√£o-lineares (c√≠rculos conc√™ntricos em alta dimens√£o)
    def create_nonlinear_clusters(n_samples_per_cluster=20, n_dims=50):
        """Cria clusters em formato de c√≠rculos conc√™ntricos"""
        clusters = []
        
        # Cluster 1: c√≠rculo interno
        angles = np.linspace(0, 2*np.pi, n_samples_per_cluster, endpoint=False)
        radius = 2
        cluster1 = np.zeros((n_samples_per_cluster, n_dims))
        cluster1[:, 0] = radius * np.cos(angles) + np.random.normal(0, 0.2, n_samples_per_cluster)
        cluster1[:, 1] = radius * np.sin(angles) + np.random.normal(0, 0.2, n_samples_per_cluster)
        cluster1[:, 2:] = np.random.normal(0, 0.1, (n_samples_per_cluster, n_dims-2))
        
        # Cluster 2: c√≠rculo externo
        radius = 5
        cluster2 = np.zeros((n_samples_per_cluster, n_dims))
        cluster2[:, 0] = radius * np.cos(angles) + np.random.normal(0, 0.3, n_samples_per_cluster)
        cluster2[:, 1] = radius * np.sin(angles) + np.random.normal(0, 0.3, n_samples_per_cluster)
        cluster2[:, 2:] = np.random.normal(0, 0.1, (n_samples_per_cluster, n_dims-2))
        
        # Cluster 3: centro
        cluster3 = np.random.normal(0, 0.5, (n_samples_per_cluster, n_dims))
        
        return cluster1, cluster2, cluster3
    
    cluster1, cluster2, cluster3 = create_nonlinear_clusters(n_samples, n_dims)
    embeddings_nonlinear = np.vstack([cluster1, cluster2, cluster3])
    labels_nonlinear = (['interno'] * n_samples + ['externo'] * n_samples + ['centro'] * n_samples)
    
    print(f"Dados n√£o-lineares: {embeddings_nonlinear.shape[0]} amostras, {embeddings_nonlinear.shape[1]} dimens√µes")
    
    # === COMPARA√á√ÉO DOS ALGORITMOS ===
    reducer = DimensionalityReducer()
    
    # Testar em dados LINEARES
    print(f"\n{'='*50}")
    print("üî¨ TESTE EM DADOS LINEARES")
    print('='*50)
    
    print("\n--- PCA em dados lineares ---")
    pca_linear = reducer.reduce_with_pca(embeddings_linear, n_components=2)
    
    print("\n--- t-SNE em dados lineares ---")
    tsne_linear = reducer.reduce_with_tsne(embeddings_linear, n_components=2)
    
    # Testar em dados N√ÉO-LINEARES
    print(f"\n{'='*50}")
    print("üî¨ TESTE EM DADOS N√ÉO-LINEARES")
    print('='*50)
    
    print("\n--- PCA em dados n√£o-lineares ---")
    pca_nonlinear = reducer.reduce_with_pca(embeddings_nonlinear, n_components=2)
    
    print("\n--- t-SNE em dados n√£o-lineares ---")
    tsne_nonlinear = reducer.reduce_with_tsne(embeddings_nonlinear, n_components=2)
    
    # === VISUALIZA√á√ïES COMPARATIVAS ===
    print(f"\n{'='*50}")
    print("üìä VISUALIZA√á√ïES COMPARATIVAS")
    print('='*50)
    
    # Dados lineares
    reducer.plot_embeddings(pca_linear, labels_linear, 
                          "PCA - Dados Lineares (Deve funcionar bem)")
    reducer.plot_embeddings(tsne_linear, labels_linear, 
                          "t-SNE - Dados Lineares (Pode n√£o ser ideal)")
    
    # Dados n√£o-lineares
    reducer.plot_embeddings(pca_nonlinear, labels_nonlinear, 
                          "PCA - Dados N√£o-Lineares (Limitado)")
    reducer.plot_embeddings(tsne_nonlinear, labels_nonlinear, 
                          "t-SNE - Dados N√£o-Lineares (Deve funcionar bem)")
    
    # === AN√ÅLISE EDUCACIONAL ===
    print(f"\n{'='*70}")

# Executar exemplo corrigido
exemplo_reducao_dimensionalidade()
```

#### QUANDO USAR CADA T√âCNICA

üîµ PCA (Principal Component Analysis):
‚úÖ MELHOR para:
    ‚Ä¢ Dados com correla√ß√µes lineares
    ‚Ä¢ Clusters bem separados linearmente
    ‚Ä¢ Quando voc√™ precisa de resultados reproduz√≠veis
    ‚Ä¢ An√°lise explorat√≥ria inicial
    ‚Ä¢ Datasets grandes (r√°pido)

‚ùå LIMITADO para:
    ‚Ä¢ Estruturas n√£o-lineares complexas
    ‚Ä¢ Clusters em formato circular/espiral
    ‚Ä¢ Quando a vari√¢ncia n√£o reflete a estrutura dos dados

üü† t-SNE (t-Distributed Stochastic Neighbor Embedding
‚úÖ MELHOR para
    ‚Ä¢ Visualiza√ß√£o de clusters complexos
    ‚Ä¢ Estruturas n√£o-lineares
    ‚Ä¢ Dados de alta dimens√£o com padr√µes ocultos
    ‚Ä¢ Identifica√ß√£o visual de agrupamentos

‚ùå LIMITADO para:
    ‚Ä¢ Datasets pequenos (<50 amostras)
    ‚Ä¢ An√°lise quantitativa de dist√¢ncias
    ‚Ä¢ Quando voc√™ precisa de resultados reproduz√≠veis
    ‚Ä¢ Dados j√° bem separados linearmente

üí° DICA PR√ÅTICA:
    1. Sempre teste PCA primeiro (r√°pido e interpret√°vel)
    2. Se PCA n√£o mostra clusters claros, tente t-SNE
    3. Ajuste perplexity do t-SNE baseado no tamanho dos dados
    4. Para produ√ß√£o, prefira PCA; para explora√ß√£o, use ambos

### üéì **Pontos-chave:**

1. **Normaliza√ß√£o √© fundamental**: Sempre normalize embeddings antes de calcular similaridades
2. **L2 √© padr√£o**: Para embeddings de texto, normaliza√ß√£o L2 √© quase sempre a melhor escolha
3. **PCA vs t-SNE**: PCA para an√°lise quantitativa, t-SNE para visualiza√ß√£o de clusters
4. **Interpreta√ß√£o cuidadosa**: t-SNE pode criar clusters visuais que n√£o existem nos dados originais
5. **Experimenta√ß√£o**: Teste diferentes t√©cnicas para encontrar a melhor para seus dados
