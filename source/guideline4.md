## 4. IntroduÃ§Ã£o ao Processamento de Linguagem Natural (NLP) e Tokens

### ğŸ¯ **Por que NLP e TokenizaÃ§Ã£o sÃ£o Fundamentais?**

O **Processamento de Linguagem Natural (NLP)** Ã© a ponte entre a linguagem humana e a compreensÃ£o computacional. Computadores nÃ£o entendem texto diretamente - eles precisam converter palavras em nÃºmeros. A **tokenizaÃ§Ã£o** Ã© o primeiro passo crucial nesse processo.

**Analogia**: Imagine que vocÃª precisa ensinar um alienÃ­gena a entender portuguÃªs. Primeiro, vocÃª dividiria as frases em palavras individuais (tokenizaÃ§Ã£o), depois explicaria o significado de cada palavra (embeddings).

### 4.1 TokenizaÃ§Ã£o

**ğŸ”‘ Conceito**: TokenizaÃ§Ã£o Ã© o processo de dividir texto em unidades menores chamadas **tokens** (palavras, subpalavras, caracteres).

```python
import nltk
from transformers import AutoTokenizer
import spacy
from typing import List

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    print("ğŸ“¥ Baixando recursos do NLTK...")
    nltk.download('punkt_tab')
    nltk.download('stopwords')
    print("âœ… Recursos baixados com sucesso!")

class TextTokenizer:
    def __init__(self):
        """
        InicializaÃ§Ã£o com diferentes tokenizadores
        
        ğŸ’¡ Dica: Execute uma vez para baixar recursos:
        nltk.download('punkt')
        nltk.download('stopwords')
        """
        # BERT tokenizer - usa subpalavras (subword tokenization)
        self.tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')
        
        # SpaCy para processamento avanÃ§ado (descomente se necessÃ¡rio)
        # self.nlp = spacy.load('en_core_web_sm')
    
    def tokenize_simple(self, text: str) -> List[str]:
        """
        ğŸŸ¢ MÃ‰TODO 1: TokenizaÃ§Ã£o Simples por EspaÃ§os
        
        âœ… Vantagens:
        - RÃ¡pido e simples
        - NÃ£o requer bibliotecas externas
        - Funciona bem para textos limpos
        
        âŒ LimitaÃ§Ãµes:
        - NÃ£o trata pontuaÃ§Ã£o adequadamente
        - NÃ£o considera contraÃ§Ãµes (don't â†’ don't, nÃ£o don + 't)
        - SensÃ­vel a espaÃ§os extras
        """
        return text.lower().split()
    
    def tokenize_nltk(self, text: str) -> List[str]:
        """
        ğŸŸ¡ MÃ‰TODO 2: TokenizaÃ§Ã£o com NLTK
        
        âœ… Vantagens:
        - Trata pontuaÃ§Ã£o corretamente
        - Separa contraÃ§Ãµes (don't â†’ ['do', "n't"])
        - Reconhece abreviaÃ§Ãµes
        - Funciona com mÃºltiplos idiomas
        
        ğŸ¯ Uso ideal: Textos gerais, anÃ¡lise linguÃ­stica bÃ¡sica
        """
        tokens = nltk.word_tokenize(text.lower())
        return tokens
    
    def tokenize_transformer(self, text: str) -> List[str]:
        """
        ğŸ”µ MÃ‰TODO 3: TokenizaÃ§Ã£o de Transformers (BERT)
        
        âœ… Vantagens:
        - Usa subpalavras (lida com palavras raras/novas)
        - Consistente com modelos prÃ©-treinados
        - VocabulÃ¡rio fixo e otimizado
        - Trata palavras fora do vocabulÃ¡rio (OOV)
        
        ğŸ¯ Uso ideal: Quando usar embeddings de transformers
        
        Exemplo: "unhappiness" â†’ ["un", "##happiness"]
        """
        tokens = self.tokenizer_bert.tokenize(text)
        return tokens
    
    def get_token_ids(self, text: str) -> List[int]:
        """
        ğŸ”¢ ConversÃ£o para IDs NumÃ©ricos
        
        Por que precisamos de IDs?
        - Modelos de ML trabalham com nÃºmeros, nÃ£o texto
        - Cada token tem um ID Ãºnico no vocabulÃ¡rio
        - Permite processamento eficiente em batches
        
        Tokens especiais do BERT:
        - [CLS]: 101 (inÃ­cio da sequÃªncia)
        - [SEP]: 102 (separador/fim)
        - [PAD]: 0 (preenchimento)
        """
        return self.tokenizer_bert.encode(text)

# ğŸš€ EXEMPLO PRÃTICO EDUCACIONAL
def demonstrar_tokenizacao():
    """DemonstraÃ§Ã£o comparativa dos mÃ©todos de tokenizaÃ§Ã£o"""
    
    tokenizer = TextTokenizer()
    
    # Texto com desafios comuns
    texto = "Embeddings sÃ£o representaÃ§Ãµes vetoriais de texto. Eles're muito Ãºteis!"
    
    print("=" * 60)
    print("ğŸ“ DEMONSTRAÃ‡ÃƒO: COMPARAÃ‡ÃƒO DE TOKENIZADORES")
    print("=" * 60)
    print(f"Texto original: '{texto}'")
    print()
    
    # Comparar mÃ©todos
    methods = [
        ("ğŸŸ¢ TokenizaÃ§Ã£o Simples", tokenizer.tokenize_simple),
        ("ğŸŸ¡ TokenizaÃ§Ã£o NLTK", tokenizer.tokenize_nltk),
        ("ğŸ”µ TokenizaÃ§Ã£o BERT", tokenizer.tokenize_transformer)
    ]
    
    for name, method in methods:
        tokens = method(texto)
        print(f"{name}:")
        print(f"   Tokens: {tokens}")
        print(f"   Quantidade: {len(tokens)} tokens")
        print()
    
    # Mostrar IDs dos tokens
    token_ids = tokenizer.get_token_ids(texto)
    print("ğŸ”¢ Token IDs (BERT):")
    print(f"   IDs: {token_ids}")
    print(f"   Quantidade: {len(token_ids)} tokens")
    
    # Explicar diferenÃ§as
    print("\n" + "=" * 60)
    print("ğŸ“Š ANÃLISE COMPARATIVA")
    print("=" * 60)
    print("ğŸŸ¢ Simples: NÃ£o separou pontuaÃ§Ã£o, manteve contraÃ§Ã£o")
    print("ğŸŸ¡ NLTK: Separou pontuaÃ§Ã£o, tratou melhor as palavras")
    print("ğŸ”µ BERT: Usou subpalavras, adicionou tokens especiais [CLS] e [SEP]")

# Executar demonstraÃ§Ã£o
demonstrar_tokenizacao()
```

```python
from transformers import AutoTokenizer
from typing import List
import re

class TextTokenizerSimplified:
    def __init__(self):
        """InicializaÃ§Ã£o sem NLTK para evitar problemas de SSL"""
        self.tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')

    def tokenize_simple(self, text: str) -> List[str]:
        """ğŸŸ¢ MÃ‰TODO 1: TokenizaÃ§Ã£o Simples por EspaÃ§os"""
        return text.lower().split()

    def tokenize_regex(self, text: str) -> List[str]:
        """ğŸŸ¡ MÃ‰TODO 2: TokenizaÃ§Ã£o com Regex (substitui NLTK)"""
        # Remove pontuaÃ§Ã£o e divide por espaÃ§os
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        tokens = text.split()
        return [token for token in tokens if token.strip()]

    def tokenize_transformer(self, text: str) -> List[str]:
        """ğŸ”µ MÃ‰TODO 3: TokenizaÃ§Ã£o de Transformers (BERT)"""
        tokens = self.tokenizer_bert.tokenize(text)
        return tokens

    def get_token_ids(self, text: str) -> List[int]:
        """ğŸ”¢ ConversÃ£o para IDs NumÃ©ricos"""
        return self.tokenizer_bert.encode(text)

def demonstrar_tokenizacao_simplificada():
    """DemonstraÃ§Ã£o sem NLTK"""
    tokenizer = TextTokenizerSimplified()
    
    texto = "Embeddings sÃ£o representaÃ§Ãµes vetoriais de texto. Eles're muito Ãºteis!"
    
    print("=" * 60)
    print("ğŸ“ DEMONSTRAÃ‡ÃƒO: COMPARAÃ‡ÃƒO DE TOKENIZADORES (SEM NLTK)")
    print("=" * 60)
    print(f"Texto original: '{texto}'")
    print()
    
    methods = [
        ("ğŸŸ¢ TokenizaÃ§Ã£o Simples", tokenizer.tokenize_simple),
        ("ğŸŸ¡ TokenizaÃ§Ã£o Regex", tokenizer.tokenize_regex),
        ("ğŸ”µ TokenizaÃ§Ã£o BERT", tokenizer.tokenize_transformer)
    ]
    
    for name, method in methods:
        tokens = method(texto)
        print(f"{name}:")
        print(f"  Tokens: {tokens}")
        print(f"  Quantidade: {len(tokens)} tokens")
        print()
    
    token_ids = tokenizer.get_token_ids(texto)
    print("ğŸ”¢ Token IDs (BERT):")
    print(f"  IDs: {token_ids}")
    print(f"  Quantidade: {len(token_ids)} tokens")

# Executar versÃ£o simplificada
demonstrar_tokenizacao_simplificada()
```

### 4.2 PrÃ©-processamento de Texto

**ğŸ¯ Objetivo**: Limpar e padronizar texto para melhorar a qualidade dos embeddings e anÃ¡lises.

```python
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from typing import List

class TextPreprocessor:
    def __init__(self, language='english'):
        """
        InicializaÃ§Ã£o com recursos de prÃ©-processamento
        
        Componentes principais:
        - Stop words: palavras muito comuns (the, and, is...)
        - Stemmer: reduz palavras ao radical (running â†’ run)
        - Lemmatizer: reduz Ã  forma canÃ´nica (better â†’ good)
        """
        self.stop_words = set(stopwords.words(language))
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()
    
    def clean_text(self, text: str) -> str:
        """
        ğŸ§¹ ETAPA 1: Limpeza BÃ¡sica do Texto
        
        OperaÃ§Ãµes realizadas:
        1. Remove caracteres especiais e nÃºmeros
        2. Converte para minÃºsculas (case normalization)
        3. Remove espaÃ§os extras
        
        âœ… Por que fazer isso?
        - Reduz ruÃ­do nos dados
        - Padroniza formato
        - Melhora consistÃªncia dos embeddings
        """
        print(f"   ğŸ“ Texto original: '{text}'")
        
        # Remover caracteres especiais e nÃºmeros
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        print(f"   ğŸ”§ ApÃ³s remoÃ§Ã£o de especiais: '{text}'")
        
        # Converter para minÃºsculas
        text = text.lower()
        print(f"   ğŸ“ ApÃ³s minÃºsculas: '{text}'")
        
        # Remover espaÃ§os extras
        text = re.sub(r'\s+', ' ', text).strip()
        print(f"   âœ¨ Texto limpo: '{text}'")
        
        return text
    
    def remove_stopwords(self, tokens: List[str]) -> List[str]:
        """
        ğŸš« ETAPA 2: RemoÃ§Ã£o de Stop Words
        
        Stop words sÃ£o palavras muito comuns que geralmente nÃ£o carregam
        significado semÃ¢ntico importante: 'the', 'and', 'is', 'in', etc.
        
        âœ… Vantagens:
        - Reduz dimensionalidade
        - Foca em palavras com mais significado
        - Melhora eficiÃªncia computacional
        
        âŒ Cuidado:
        - Pode remover contexto importante em algumas tarefas
        - "Not good" â†’ "good" (perde negaÃ§Ã£o)
        """
        original_count = len(tokens)
        filtered_tokens = [token for token in tokens if token not in self.stop_words]
        removed_count = original_count - len(filtered_tokens)
        
        print(f"   ğŸš« Removidas {removed_count} stop words de {original_count} tokens")
        print(f"   ğŸ“‹ Tokens restantes: {filtered_tokens}")
        
        return filtered_tokens
    
    def stem_tokens(self, tokens: List[str]) -> List[str]:
        """
        ğŸŒ± ETAPA 3A: Stemming (Alternativa 1)
        
        Stemming remove sufixos para encontrar o "radical" da palavra.
        Algoritmo: Porter Stemmer (mais comum)
        
        Exemplos:
        - running, runs, ran â†’ run
        - better, good â†’ better, good (nÃ£o conecta palavras relacionadas)
        
        âœ… Vantagens: RÃ¡pido, simples
        âŒ LimitaÃ§Ãµes: Pode gerar palavras inexistentes, menos preciso
        """
        stemmed = [self.stemmer.stem(token) for token in tokens]
        print(f"   ğŸŒ± Stemming aplicado:")
        for original, stemmed_word in zip(tokens, stemmed):
            if original != stemmed_word:
                print(f"      {original} â†’ {stemmed_word}")
        return stemmed
    
    def lemmatize_tokens(self, tokens: List[str]) -> List[str]:
        """
        ğŸ“š ETAPA 3B: LemmatizaÃ§Ã£o (Alternativa 2 - Recomendada)
        
        LemmatizaÃ§Ã£o reduz palavras Ã  sua forma canÃ´nica (lemma) usando
        conhecimento linguÃ­stico e dicionÃ¡rios.
        
        Exemplos:
        - running, runs, ran â†’ run
        - better â†’ good
        - mice â†’ mouse
        
        âœ… Vantagens: Mais preciso, gera palavras reais
        âŒ LimitaÃ§Ãµes: Mais lento, requer recursos linguÃ­sticos
        """
        lemmatized = [self.lemmatizer.lemmatize(token) for token in tokens]
        print(f"   ğŸ“š LemmatizaÃ§Ã£o aplicada:")
        for original, lemma in zip(tokens, lemmatized):
            if original != lemma:
                print(f"      {original} â†’ {lemma}")
        return lemmatized
    
    def preprocess_pipeline(self, text: str) -> List[str]:
        """
        ğŸ”„ Pipeline Completo de PrÃ©-processamento
        
        Ordem das operaÃ§Ãµes (importante!):
        1. Limpeza â†’ 2. TokenizaÃ§Ã£o â†’ 3. Stop words â†’ 4. LemmatizaÃ§Ã£o
        
        ğŸ’¡ Dica: A ordem importa! Limpe antes de tokenizar,
        remova stop words antes de lemmatizar.
        """
        print(f"\nğŸ”„ INICIANDO PIPELINE DE PRÃ‰-PROCESSAMENTO")
        print("=" * 50)
        
        # Etapa 1: Limpeza
        print("\nğŸ§¹ ETAPA 1: LIMPEZA")
        clean_text = self.clean_text(text)
        
        # Etapa 2: TokenizaÃ§Ã£o simples
        print("\nâœ‚ï¸ ETAPA 2: TOKENIZAÃ‡ÃƒO")
        tokens = clean_text.split()
        print(f"   ğŸ“ Tokens: {tokens}")
        
        # Etapa 3: RemoÃ§Ã£o de stop words
        print("\nğŸš« ETAPA 3: REMOÃ‡ÃƒO DE STOP WORDS")
        tokens = self.remove_stopwords(tokens)
        
        # Etapa 4: LemmatizaÃ§Ã£o
        print("\nğŸ“š ETAPA 4: LEMMATIZAÃ‡ÃƒO")
        tokens = self.lemmatize_tokens(tokens)
        
        print(f"\nâœ… RESULTADO FINAL: {tokens}")
        return tokens

# ğŸš€ EXEMPLO PRÃTICO EDUCACIONAL
def demonstrar_preprocessamento():
    """DemonstraÃ§Ã£o completa do prÃ©-processamento"""
    
    preprocessor = TextPreprocessor()
    
    # Texto com vÃ¡rios desafios
    texto_exemplo = """
    The running dogs are better than cats! 
    They're playing in the beautiful gardens.
    """
    
    print("ğŸ“ DEMONSTRAÃ‡ÃƒO: PRÃ‰-PROCESSAMENTO DE TEXTO")
    print("=" * 60)
    
    # Executar pipeline completo
    resultado = preprocessor.preprocess_pipeline(texto_exemplo)
    
    print("\nğŸ“Š RESUMO DO PROCESSAMENTO:")
    print("=" * 30)
    print(f"ğŸ“ Texto original: '{texto_exemplo.strip()}'")
    print(f"âœ… Tokens finais: {resultado}")
    print(f"ğŸ“Š ReduÃ§Ã£o: {len(texto_exemplo.split())} â†’ {len(resultado)} tokens")
    
    # Comparar stemming vs lemmatizaÃ§Ã£o
    print("\nğŸ” COMPARAÃ‡ÃƒO: STEMMING vs LEMMATIZAÃ‡ÃƒO")
    print("=" * 40)
    
    tokens_exemplo = ['running', 'better', 'playing', 'beautiful']
    
    for token in tokens_exemplo:
        stemmed = preprocessor.stemmer.stem(token)
        lemmatized = preprocessor.lemmatizer.lemmatize(token)
        print(f"{token:10} â†’ Stem: {stemmed:8} | Lemma: {lemmatized}")

# Executar demonstraÃ§Ã£o
demonstrar_preprocessamento()
```

```python
import re
from typing import List

class TextPreprocessorSimplified:
    def __init__(self, language='english'):
        """
        InicializaÃ§Ã£o com recursos de prÃ©-processamento simplificados
        
        Componentes principais:
        - Stop words: lista bÃ¡sica de palavras comuns
        - Stemming simples: remoÃ§Ã£o de sufixos bÃ¡sicos
        - Sem dependÃªncias externas
        """
        # Lista bÃ¡sica de stop words em inglÃªs
        self.stop_words = {
            'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
            'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
            'to', 'was', 'will', 'with', 'they', 'their', 'them', 'this',
            'these', 'those', 'we', 'you', 'your', 'i', 'me', 'my', 'mine',
            'our', 'ours', 'she', 'her', 'hers', 'him', 'his'
        }
        
        # Regras bÃ¡sicas de stemming (sufixos comuns)
        self.stemming_rules = [
            ('ing', ''),      # running â†’ runn
            ('ly', ''),       # quickly â†’ quick
            ('ed', ''),       # played â†’ play
            ('ies', 'y'),     # flies â†’ fly
            ('ied', 'y'),     # tried â†’ try
            ('ies', 'y'),     # studies â†’ study
            ('s', ''),        # dogs â†’ dog
        ]
    
    def clean_text(self, text: str) -> str:
        """
        ğŸ§¹ ETAPA 1: Limpeza BÃ¡sica do Texto
        
        OperaÃ§Ãµes realizadas:
        1. Remove caracteres especiais e nÃºmeros
        2. Converte para minÃºsculas (case normalization)
        3. Remove espaÃ§os extras
        
        âœ… Por que fazer isso?
        - Reduz ruÃ­do nos dados
        - Padroniza formato
        - Melhora consistÃªncia dos embeddings
        """
        print(f"   ğŸ“ Texto original: '{text}'")
        
        # Remover caracteres especiais e nÃºmeros
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        print(f"   ğŸ”§ ApÃ³s remoÃ§Ã£o de especiais: '{text}'")
        
        # Converter para minÃºsculas
        text = text.lower()
        print(f"   ğŸ“ ApÃ³s minÃºsculas: '{text}'")
        
        # Remover espaÃ§os extras
        text = re.sub(r'\s+', ' ', text).strip()
        print(f"   âœ¨ Texto limpo: '{text}'")
        
        return text
    
    def remove_stopwords(self, tokens: List[str]) -> List[str]:
        """
        ğŸš« ETAPA 2: RemoÃ§Ã£o de Stop Words
        
        Stop words sÃ£o palavras muito comuns que geralmente nÃ£o carregam
        significado semÃ¢ntico importante: 'the', 'and', 'is', 'in', etc.
        
        âœ… Vantagens:
        - Reduz dimensionalidade
        - Foca em palavras com mais significado
        - Melhora eficiÃªncia computacional
        
        âŒ Cuidado:
        - Pode remover contexto importante em algumas tarefas
        - "Not good" â†’ "good" (perde negaÃ§Ã£o)
        """
        original_count = len(tokens)
        filtered_tokens = [token for token in tokens if token not in self.stop_words]
        removed_count = original_count - len(filtered_tokens)
        
        print(f"   ğŸš« Removidas {removed_count} stop words de {original_count} tokens")
        print(f"   ğŸ“‹ Tokens restantes: {filtered_tokens}")
        
        return filtered_tokens
    
    def simple_stem(self, word: str) -> str:
        """
        ğŸŒ± Stemming Simples com Regras BÃ¡sicas
        
        Aplica regras simples de remoÃ§Ã£o de sufixos.
        NÃ£o Ã© tÃ£o preciso quanto Porter Stemmer, mas funciona sem dependÃªncias.
        
        Exemplos:
        - running â†’ runn
        - quickly â†’ quick
        - played â†’ play
        """
        for suffix, replacement in self.stemming_rules:
            if word.endswith(suffix) and len(word) > len(suffix) + 2:
                return word[:-len(suffix)] + replacement
        return word
    
    def stem_tokens(self, tokens: List[str]) -> List[str]:
        """
        ğŸŒ± ETAPA 3A: Stemming Simples (Alternativa 1)
        
        Stemming remove sufixos para encontrar o "radical" da palavra.
        VersÃ£o simplificada com regras bÃ¡sicas.
        
        âœ… Vantagens: RÃ¡pido, sem dependÃªncias externas
        âŒ LimitaÃ§Ãµes: Menos preciso que algoritmos avanÃ§ados
        """
        stemmed = [self.simple_stem(token) for token in tokens]
        print(f"   ğŸŒ± Stemming simples aplicado:")
        for original, stemmed_word in zip(tokens, stemmed):
            if original != stemmed_word:
                print(f"      {original} â†’ {stemmed_word}")
        return stemmed
    
    def simple_lemmatize(self, word: str) -> str:
        """
        ğŸ“š LemmatizaÃ§Ã£o Simples com DicionÃ¡rio BÃ¡sico
        
        VersÃ£o simplificada usando um pequeno dicionÃ¡rio de formas irregulares.
        """
        # DicionÃ¡rio bÃ¡sico de formas irregulares comuns
        irregular_forms = {
            'better': 'good',
            'best': 'good',
            'worse': 'bad',
            'worst': 'bad',
            'mice': 'mouse',
            'children': 'child',
            'feet': 'foot',
            'teeth': 'tooth',
            'men': 'man',
            'women': 'woman',
            'running': 'run',
            'ran': 'run',
            'swimming': 'swim',
            'swam': 'swim',
            'flying': 'fly',
            'flew': 'fly'
        }
        
        return irregular_forms.get(word, word)
    
    def lemmatize_tokens(self, tokens: List[str]) -> List[str]:
        """
        ğŸ“š ETAPA 3B: LemmatizaÃ§Ã£o Simples (Alternativa 2 - Recomendada)
        
        LemmatizaÃ§Ã£o reduz palavras Ã  sua forma canÃ´nica usando
        um dicionÃ¡rio bÃ¡sico de formas irregulares.
        
        âœ… Vantagens: Mais preciso que stemming simples
        âŒ LimitaÃ§Ãµes: DicionÃ¡rio limitado, menos abrangente
        """
        lemmatized = [self.simple_lemmatize(token) for token in tokens]
        print(f"   ğŸ“š LemmatizaÃ§Ã£o simples aplicada:")
        for original, lemma in zip(tokens, lemmatized):
            if original != lemma:
                print(f"      {original} â†’ {lemma}")
        return lemmatized
    
    def preprocess_pipeline(self, text: str) -> List[str]:
        """
        ğŸ”„ Pipeline Completo de PrÃ©-processamento Simplificado
        
        Ordem das operaÃ§Ãµes (importante!):
        1. Limpeza â†’ 2. TokenizaÃ§Ã£o â†’ 3. Stop words â†’ 4. LemmatizaÃ§Ã£o
        
        ğŸ’¡ Dica: A ordem importa! Limpe antes de tokenizar,
        remova stop words antes de lemmatizar.
        """
        print(f"\nğŸ”„ INICIANDO PIPELINE DE PRÃ‰-PROCESSAMENTO SIMPLIFICADO")
        print("=" * 60)
        
        # Etapa 1: Limpeza
        print("\nğŸ§¹ ETAPA 1: LIMPEZA")
        clean_text = self.clean_text(text)
        
        # Etapa 2: TokenizaÃ§Ã£o simples
        print("\nâœ‚ï¸ ETAPA 2: TOKENIZAÃ‡ÃƒO")
        tokens = clean_text.split()
        print(f"   ğŸ“ Tokens: {tokens}")
        
        # Etapa 3: RemoÃ§Ã£o de stop words
        print("\nğŸš« ETAPA 3: REMOÃ‡ÃƒO DE STOP WORDS")
        tokens = self.remove_stopwords(tokens)
        
        # Etapa 4: LemmatizaÃ§Ã£o simples
        print("\nğŸ“š ETAPA 4: LEMMATIZAÃ‡ÃƒO SIMPLES")
        tokens = self.lemmatize_tokens(tokens)
        
        print(f"\nâœ… RESULTADO FINAL: {tokens}")
        return tokens

# ğŸš€ EXEMPLO PRÃTICO EDUCACIONAL
def demonstrar_preprocessamento_simplificado():
    """DemonstraÃ§Ã£o completa do prÃ©-processamento sem NLTK"""
    
    preprocessor = TextPreprocessorSimplified()
    
    # Texto com vÃ¡rios desafios
    texto_exemplo = """
    The running dogs are better than cats! 
    They're playing in the beautiful gardens.
    """
    
    print("ğŸ“ DEMONSTRAÃ‡ÃƒO: PRÃ‰-PROCESSAMENTO DE TEXTO (VERSÃƒO SIMPLIFICADA)")
    print("=" * 70)
    
    # Executar pipeline completo
    resultado = preprocessor.preprocess_pipeline(texto_exemplo)
    
    print("\nğŸ“Š RESUMO DO PROCESSAMENTO:")
    print("=" * 30)
    print(f"ğŸ“ Texto original: '{texto_exemplo.strip()}'")
    print(f"âœ… Tokens finais: {resultado}")
    print(f"ğŸ“Š ReduÃ§Ã£o: {len(texto_exemplo.split())} â†’ {len(resultado)} tokens")
    
    # Comparar stemming vs lemmatizaÃ§Ã£o simples
    print("\nğŸ” COMPARAÃ‡ÃƒO: STEMMING vs LEMMATIZAÃ‡ÃƒO SIMPLES")
    print("=" * 50)
    
    tokens_exemplo = ['running', 'better', 'playing', 'beautiful', 'quickly']
    
    for token in tokens_exemplo:
        stemmed = preprocessor.simple_stem(token)
        lemmatized = preprocessor.simple_lemmatize(token)
        print(f"{token:10} â†’ Stem: {stemmed:8} | Lemma: {lemmatized}")

# Executar demonstraÃ§Ã£o
demonstrar_preprocessamento_simplificado()
```

### ğŸ¯ **Pontos-Chave para FixaÃ§Ã£o**

1. **TokenizaÃ§Ã£o Ã© fundamental**: Ã‰ o primeiro passo para converter texto em dados processÃ¡veis
2. **Diferentes mÃ©todos, diferentes propÃ³sitos**: Simples para prototipagem, NLTK para anÃ¡lise geral, Transformers para modelos modernos
3. **PrÃ©-processamento melhora qualidade**: Texto limpo gera embeddings mais consistentes
4. **Ordem importa**: Sempre siga a sequÃªncia lÃ³gica de processamento
5. **Trade-offs**: Mais processamento = mais lento, mas geralmente melhor qualidade

### ğŸ’¡ **Dicas PrÃ¡ticas**

- **Para embeddings**: Use tokenizaÃ§Ã£o compatÃ­vel com o modelo escolhido
- **Para anÃ¡lise exploratÃ³ria**: NLTK Ã© uma boa escolha
- **Para produÃ§Ã£o**: Considere performance vs. qualidade
- **Sempre valide**: Inspecione os resultados de cada etapa
