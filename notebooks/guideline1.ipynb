{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9a8f38",
   "metadata": {},
   "source": [
    "## 1. Recap sobre Embeddings\n",
    "\n",
    "### 1.1 Conceitos Fundamentais\n",
    "Embeddings s√£o representa√ß√µes num√©ricas densas que capturam o significado sem√¢ntico de palavras, frases ou documentos em um espa√ßo vetorial de dimens√£o fixa. Eles resolvem limita√ß√µes das representa√ß√µes tradicionais como one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c703c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARA√á√ÉO DE REPRESENTA√á√ïES ===\n",
      "One-hot 'gato': [1, 0, 0, 0]\n",
      "Embedding 'gato': [0.9 0.6 0.4 0.9]\n",
      "\n",
      "Tamanho do vocabul√°rio: 4\n",
      "Dimens√µes one-hot: 4 (esparso: 3/4 zeros)\n",
      "Dimens√µes embedding: 4 (denso: todos valores n√£o-zero)\n"
     ]
    }
   ],
   "source": [
    "# Exemplo conceitual: representa√ß√£o de palavras como vetores\n",
    "import numpy as np\n",
    "\n",
    "# Representa√ß√£o tradicional (one-hot encoding)\n",
    "vocabulario = [\"gato\", \"cachorro\", \"animal\", \"felino\"]\n",
    "gato_onehot = [1, 0, 0, 0]\n",
    "cachorro_onehot = [0, 1, 0, 0]\n",
    "\n",
    "# Representa√ß√£o com embeddings (vetores densos)\n",
    "gato_embedding = np.array([1.0, 0.6, 0.4, 0.9])\n",
    "cachorro_embedding = np.array([0.6, 1.0, 0.4, 0.7])\n",
    "\n",
    "print(\"=== COMPARA√á√ÉO DE REPRESENTA√á√ïES ===\")\n",
    "print(f\"One-hot 'gato': {gato_onehot}\")\n",
    "print(f\"Embedding 'gato': {gato_embedding}\")\n",
    "print(f\"\\nTamanho do vocabul√°rio: {len(vocabulario)}\")\n",
    "print(f\"Dimens√µes one-hot: {len(gato_onehot)} (esparso: {gato_onehot.count(0)}/{len(gato_onehot)} zeros)\")\n",
    "print(f\"Dimens√µes embedding: {len(gato_embedding)} (denso: todos valores n√£o-zero)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf82a02",
   "metadata": {},
   "source": [
    "**Principais diferen√ßas:**\n",
    "- **One-hot encoding**: Vetores esparsos com apenas um 1 e muitos 0s\n",
    "- **Embeddings**: Vetores densos com valores reais que capturam rela√ß√µes sem√¢nticas\n",
    "- **Dimensionalidade**: One-hot cresce com o vocabul√°rio; embeddings t√™m tamanho fixo\n",
    "\n",
    "### 1.2 Vantagens dos Embeddings\n",
    "\n",
    "Os embeddings permitem que **palavras semanticamente similares tenham representa√ß√µes pr√≥ximas** no espa√ßo vetorial, algo imposs√≠vel com one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d6fa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AN√ÅLISE DE SIMILARIDADE SEM√ÇNTICA ===\n",
      "Similaridade gato ‚Üî felino: 0.986\n",
      "Similaridade gato ‚Üî cachorro: 0.915\n",
      "Similaridade gato ‚Üî carro: 0.235\n",
      "\n",
      "=== INTERPRETA√á√ÉO ===\n",
      "‚úì 'Gato' √© mais similar a 'felino' (mesmo conceito)\n",
      "‚úì 'Gato' √© mais similar a 'cachorro' (ambos animais) que a 'carro'\n",
      "\n",
      "=== COMPARA√á√ÉO: ONE-HOT vs EMBEDDINGS ===\n",
      "Similaridade one-hot gato ‚Üî felino: 0.000\n",
      "‚ùå One-hot n√£o captura que 'gato' e 'felino' s√£o relacionados!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Demonstra√ß√£o de similaridade sem√¢ntica\n",
    "embeddings = {\n",
    "    \"gato\": np.array([1.0, 0.6, 0.1, 0.85]),\n",
    "    \"felino\": np.array([0.85, 0.75, 0.1, 1.0]),\n",
    "    \"cachorro\": np.array([0.6, 1.0, 0.1, 0.7]),\n",
    "    \"carro\": np.array([0.1, 0.1, 1.0, 0.1])\n",
    "}\n",
    "\n",
    "# Calculando similaridade\n",
    "sim_gato_felino = cosine_similarity([embeddings[\"gato\"]], [embeddings[\"felino\"]])[0][0]\n",
    "sim_gato_cachorro = cosine_similarity([embeddings[\"gato\"]], [embeddings[\"cachorro\"]])[0][0]\n",
    "sim_gato_carro = cosine_similarity([embeddings[\"gato\"]], [embeddings[\"carro\"]])[0][0]\n",
    "\n",
    "print(\"=== AN√ÅLISE DE SIMILARIDADE SEM√ÇNTICA ===\")\n",
    "print(f\"Similaridade gato ‚Üî felino: {sim_gato_felino:.3f}\")\n",
    "print(f\"Similaridade gato ‚Üî cachorro: {sim_gato_cachorro:.3f}\")\n",
    "print(f\"Similaridade gato ‚Üî carro: {sim_gato_carro:.3f}\")\n",
    "\n",
    "# Interpreta√ß√£o dos resultados\n",
    "print(\"\\n=== INTERPRETA√á√ÉO ===\")\n",
    "if sim_gato_felino > sim_gato_cachorro:\n",
    "    print(\"‚úì 'Gato' √© mais similar a 'felino' (mesmo conceito)\")\n",
    "if sim_gato_cachorro > sim_gato_carro:\n",
    "    print(\"‚úì 'Gato' √© mais similar a 'cachorro' (ambos animais) que a 'carro'\")\n",
    "\n",
    "# Demonstra√ß√£o com one-hot (para compara√ß√£o)\n",
    "print(\"\\n=== COMPARA√á√ÉO: ONE-HOT vs EMBEDDINGS ===\")\n",
    "onehot_gato = [1, 0, 0, 0]\n",
    "onehot_felino = [0, 0, 0, 1]  # Posi√ß√£o diferente no vocabul√°rio\n",
    "onehot_similarity = cosine_similarity([onehot_gato], [onehot_felino])[0][0]\n",
    "print(f\"Similaridade one-hot gato ‚Üî felino: {onehot_similarity:.3f}\")\n",
    "print(\"‚ùå One-hot n√£o captura que 'gato' e 'felino' s√£o relacionados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d41da7",
   "metadata": {},
   "source": [
    "### 1.3 Propriedades Matem√°ticas dos Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89822dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROPRIEDADES MATEM√ÅTICAS ===\n",
      "gato:\n",
      "  Vetor: [0.2 0.8 0.1 0.9]\n",
      "  Norma (magnitude): 1.225\n",
      "  Normalizado: [0.16329932 0.65319726 0.08164966 0.73484692]\n",
      "\n",
      "felino:\n",
      "  Vetor: [0.25 0.75 0.15 0.85]\n",
      "  Norma (magnitude): 1.170\n",
      "  Normalizado: [0.21358941 0.64076824 0.12815365 0.72620401]\n",
      "\n",
      "cachorro:\n",
      "  Vetor: [0.3 0.7 0.2 0.8]\n",
      "  Norma (magnitude): 1.122\n",
      "  Normalizado: [0.26726124 0.62360956 0.17817416 0.71269665]\n",
      "\n",
      "carro:\n",
      "  Vetor: [0.9 0.1 0.8 0.2]\n",
      "  Norma (magnitude): 1.225\n",
      "  Normalizado: [0.73484692 0.08164966 0.65319726 0.16329932]\n",
      "\n",
      "=== OPERA√á√ïES VETORIAIS ===\n",
      "Vetor analogia (felino - gato + cachorro): [0.35 0.65 0.25 0.75]\n",
      "Palavra mais pr√≥xima da analogia: felino (similaridade: 0.987)\n"
     ]
    }
   ],
   "source": [
    "# Demonstra√ß√£o de propriedades matem√°ticas\n",
    "print(\"=== PROPRIEDADES MATEM√ÅTICAS ===\")\n",
    "\n",
    "def calculate_vector_properties(name, vector):\n",
    "    norm = np.linalg.norm(vector)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Vetor: {vector}\")\n",
    "    print(f\"  Norma (magnitude): {norm:.3f}\")\n",
    "    print(f\"  Normalizado: {vector/norm}\")\n",
    "    print()\n",
    "\n",
    "for name, embedding in embeddings.items():\n",
    "    calculate_vector_properties(name, embedding)\n",
    "\n",
    "# Opera√ß√µes vetoriais\n",
    "print(\"=== OPERA√á√ïES VETORIAIS ===\")\n",
    "# Analogia: gato est√° para felino assim como cachorro est√° para...?\n",
    "analogy_vector = embeddings[\"felino\"] - embeddings[\"gato\"] + embeddings[\"cachorro\"]\n",
    "print(f\"Vetor analogia (felino - gato + cachorro): {analogy_vector}\")\n",
    "\n",
    "# Encontrar palavra mais pr√≥xima da analogia\n",
    "similarities_analogy = {}\n",
    "for word, vec in embeddings.items():\n",
    "    if word != \"cachorro\":  # Excluir a palavra de entrada\n",
    "        sim = cosine_similarity([analogy_vector], [vec])[0][0]\n",
    "        similarities_analogy[word] = sim\n",
    "\n",
    "closest_word = max(similarities_analogy, key=similarities_analogy.get)\n",
    "print(f\"Palavra mais pr√≥xima da analogia: {closest_word} (similaridade: {similarities_analogy[closest_word]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b98da",
   "metadata": {},
   "source": [
    "### 1.4 Vantagens dos Embeddings - Resumo\n",
    "\n",
    "#### üéØ **Principais Vantagens dos Embeddings**\n",
    "\n",
    "1. **üéØ Capturam similaridade sem√¢ntica**\n",
    "   - Palavras com significados similares t√™m representa√ß√µes pr√≥ximas no espa√ßo vetorial\n",
    "   - Exemplo: \"gato\" e \"felino\" ter√£o alta similaridade cosseno\n",
    "\n",
    "2. **üìè Dimensionalidade fixa**\n",
    "   - Independente do tamanho do vocabul√°rio\n",
    "   - One-hot: cresce com vocabul√°rio | Embeddings: tamanho fixo (ex: 300D)\n",
    "\n",
    "3. **üî¢ Representa√ß√£o densa**\n",
    "   - Todos os valores s√£o significativos (n√£o h√° zeros desnecess√°rios)\n",
    "   - Uso eficiente do espa√ßo de mem√≥ria\n",
    "\n",
    "4. **üßÆ Opera√ß√µes matem√°ticas significativas**\n",
    "   - Analogias: \"rei\" - \"homem\" + \"mulher\" ‚âà \"rainha\"\n",
    "   - Aritm√©tica vetorial com significado sem√¢ntico\n",
    "\n",
    "5. **üöÄ Efici√™ncia computacional**\n",
    "   - Processamento mais r√°pido em modelos de Machine Learning\n",
    "   - Menor dimensionalidade que one-hot para vocabul√°rios grandes\n",
    "\n",
    "6. **üîÑ Transferibilidade entre tarefas**\n",
    "   - Embeddings pr√©-treinados podem ser reutilizados\n",
    "   - Word2Vec, GloVe, FastText servem para m√∫ltiplas aplica√ß√µes\n",
    "\n",
    "#### üõ†Ô∏è **Aplica√ß√µes Pr√°ticas**\n",
    "\n",
    "- **Sistemas de recomenda√ß√£o**: Encontrar produtos/conte√∫dos similares\n",
    "- **Busca sem√¢ntica**: Buscar por significado, n√£o apenas palavras-chave\n",
    "- **Tradu√ß√£o autom√°tica**: Mapear conceitos entre idiomas\n",
    "- **An√°lise de sentimentos**: Capturar nuances emocionais\n",
    "- **Classifica√ß√£o de textos**: Categoriza√ß√£o autom√°tica de documentos\n",
    "- **Chatbots e assistentes virtuais**: Compreens√£o de inten√ß√µes do usu√°rio\n",
    "\n",
    "#### üîë **Conceitos-chave para Fixa√ß√£o**\n",
    "\n",
    "- **Similaridade Cosseno**: Mede o √¢ngulo entre vetores (0 = ortogonais, 1 = id√™nticos)\n",
    "- **Espa√ßo Vetorial**: Embeddings vivem em um espa√ßo onde dist√¢ncia = similaridade sem√¢ntica\n",
    "- **Densidade**: Cada dimens√£o contribui para o significado (vs. one-hot com muitos zeros)\n",
    "- **Aprendizado**: Embeddings s√£o aprendidos automaticamente a partir de grandes corpus de texto"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
