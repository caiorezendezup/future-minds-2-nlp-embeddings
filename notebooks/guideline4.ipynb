{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655ab193",
   "metadata": {},
   "source": [
    "## 4. Introdu√ß√£o ao Processamento de Linguagem Natural (NLP) e Tokens\n",
    "\n",
    "### üéØ **Por que NLP e Tokeniza√ß√£o s√£o Fundamentais?**\n",
    "\n",
    "O **Processamento de Linguagem Natural (NLP)** √© a ponte entre a linguagem humana e a compreens√£o computacional. Computadores n√£o entendem texto diretamente - eles precisam converter palavras em n√∫meros. A **tokeniza√ß√£o** √© o primeiro passo crucial nesse processo.\n",
    "\n",
    "**Analogia**: Imagine que voc√™ precisa ensinar um alien√≠gena a entender portugu√™s. Primeiro, voc√™ dividiria as frases em palavras individuais (tokeniza√ß√£o), depois explicaria o significado de cada palavra (embeddings).\n",
    "\n",
    "### 4.1 Tokeniza√ß√£o\n",
    "\n",
    "**üîë Conceito**: Tokeniza√ß√£o √© o processo de dividir texto em unidades menores chamadas **tokens** (palavras, subpalavras, caracteres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78e0f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéì DEMONSTRA√á√ÉO: COMPARA√á√ÉO DE TOKENIZADORES (SEM NLTK)\n",
      "============================================================\n",
      "Texto original: 'Embeddings s√£o representa√ß√µes vetoriais de texto. Eles're muito √∫teis!'\n",
      "\n",
      "üü¢ Tokeniza√ß√£o Simples:\n",
      "  Tokens: ['embeddings', 's√£o', 'representa√ß√µes', 'vetoriais', 'de', 'texto.', \"eles're\", 'muito', '√∫teis!']\n",
      "  Quantidade: 9 tokens\n",
      "\n",
      "üü° Tokeniza√ß√£o Regex:\n",
      "  Tokens: ['embeddings', 's√£o', 'representa√ß√µes', 'vetoriais', 'de', 'texto', 'eles', 're', 'muito', '√∫teis']\n",
      "  Quantidade: 10 tokens\n",
      "\n",
      "üîµ Tokeniza√ß√£o BERT:\n",
      "  Tokens: ['em', '##bed', '##ding', '##s', 'sao', 'represent', '##aco', '##es', 'veto', '##ria', '##is', 'de', 'text', '##o', '.', 'el', '##es', \"'\", 're', 'mu', '##ito', 'ut', '##eis', '!']\n",
      "  Quantidade: 24 tokens\n",
      "\n",
      "üî¢ Token IDs (BERT):\n",
      "  IDs: [101, 7861, 8270, 4667, 2015, 7509, 5050, 22684, 2229, 22102, 4360, 2483, 2139, 3793, 2080, 1012, 3449, 2229, 1005, 2128, 14163, 9956, 21183, 17580, 999, 102]\n",
      "  Quantidade: 26 tokens\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "class TextTokenizerSimplified:\n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializa√ß√£o sem NLTK para evitar problemas de SSL\"\"\"\n",
    "        self.tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def tokenize_simple(self, text: str) -> List[str]:\n",
    "        \"\"\"üü¢ M√âTODO 1: Tokeniza√ß√£o Simples por Espa√ßos\"\"\"\n",
    "        return text.lower().split()\n",
    "\n",
    "    def tokenize_regex(self, text: str) -> List[str]:\n",
    "        \"\"\"üü° M√âTODO 2: Tokeniza√ß√£o com Regex (substitui NLTK)\"\"\"\n",
    "        # Remove pontua√ß√£o e divide por espa√ßos\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "        tokens = text.split()\n",
    "        return [token for token in tokens if token.strip()]\n",
    "\n",
    "    def tokenize_transformer(self, text: str) -> List[str]:\n",
    "        \"\"\"üîµ M√âTODO 3: Tokeniza√ß√£o de Transformers (BERT)\"\"\"\n",
    "        tokens = self.tokenizer_bert.tokenize(text)\n",
    "        return tokens\n",
    "\n",
    "    def get_token_ids(self, text: str) -> List[int]:\n",
    "        \"\"\"üî¢ Convers√£o para IDs Num√©ricos\"\"\"\n",
    "        return self.tokenizer_bert.encode(text)\n",
    "\n",
    "def demonstrar_tokenizacao_simplificada():\n",
    "    \"\"\"Demonstra√ß√£o sem NLTK\"\"\"\n",
    "    tokenizer = TextTokenizerSimplified()\n",
    "    \n",
    "    texto = \"Embeddings s√£o representa√ß√µes vetoriais de texto. Eles're muito √∫teis!\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üéì DEMONSTRA√á√ÉO: COMPARA√á√ÉO DE TOKENIZADORES (SEM NLTK)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Texto original: '{texto}'\")\n",
    "    print()\n",
    "    \n",
    "    methods = [\n",
    "        (\"üü¢ Tokeniza√ß√£o Simples\", tokenizer.tokenize_simple),\n",
    "        (\"üü° Tokeniza√ß√£o Regex\", tokenizer.tokenize_regex),\n",
    "        (\"üîµ Tokeniza√ß√£o BERT\", tokenizer.tokenize_transformer)\n",
    "    ]\n",
    "    \n",
    "    for name, method in methods:\n",
    "        tokens = method(texto)\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Tokens: {tokens}\")\n",
    "        print(f\"  Quantidade: {len(tokens)} tokens\")\n",
    "        print()\n",
    "    \n",
    "    token_ids = tokenizer.get_token_ids(texto)\n",
    "    print(\"üî¢ Token IDs (BERT):\")\n",
    "    print(f\"  IDs: {token_ids}\")\n",
    "    print(f\"  Quantidade: {len(token_ids)} tokens\")\n",
    "\n",
    "# Executar vers√£o simplificada\n",
    "demonstrar_tokenizacao_simplificada()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afed4d9",
   "metadata": {},
   "source": [
    "### 4.2 Pr√©-processamento de Texto\n",
    "\n",
    "**üéØ Objetivo**: Limpar e padronizar texto para melhorar a qualidade dos embeddings e an√°lises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "154eccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì DEMONSTRA√á√ÉO: PR√â-PROCESSAMENTO DE TEXTO (VERS√ÉO SIMPLIFICADA)\n",
      "======================================================================\n",
      "\n",
      "üîÑ INICIANDO PIPELINE DE PR√â-PROCESSAMENTO SIMPLIFICADO\n",
      "============================================================\n",
      "\n",
      "üßπ ETAPA 1: LIMPEZA\n",
      "   üìù Texto original: '\n",
      "    The running dogs are better than cats! \n",
      "    They're playing in the beautiful gardens.\n",
      "    '\n",
      "   üîß Ap√≥s remo√ß√£o de especiais: '\n",
      "    The running dogs are better than cats \n",
      "    Theyre playing in the beautiful gardens\n",
      "    '\n",
      "   üìù Ap√≥s min√∫sculas: '\n",
      "    the running dogs are better than cats \n",
      "    theyre playing in the beautiful gardens\n",
      "    '\n",
      "   ‚ú® Texto limpo: 'the running dogs are better than cats theyre playing in the beautiful gardens'\n",
      "\n",
      "‚úÇÔ∏è ETAPA 2: TOKENIZA√á√ÉO\n",
      "   üìù Tokens: ['the', 'running', 'dogs', 'are', 'better', 'than', 'cats', 'theyre', 'playing', 'in', 'the', 'beautiful', 'gardens']\n",
      "\n",
      "üö´ ETAPA 3: REMO√á√ÉO DE STOP WORDS\n",
      "   üö´ Removidas 4 stop words de 13 tokens\n",
      "   üìã Tokens restantes: ['running', 'dogs', 'better', 'than', 'cats', 'theyre', 'playing', 'beautiful', 'gardens']\n",
      "\n",
      "üìö ETAPA 4: LEMMATIZA√á√ÉO SIMPLES\n",
      "   üìö Lemmatiza√ß√£o simples aplicada:\n",
      "      running ‚Üí run\n",
      "      better ‚Üí good\n",
      "\n",
      "‚úÖ RESULTADO FINAL: ['run', 'dogs', 'good', 'than', 'cats', 'theyre', 'playing', 'beautiful', 'gardens']\n",
      "\n",
      "üìä RESUMO DO PROCESSAMENTO:\n",
      "==============================\n",
      "üìù Texto original: 'The running dogs are better than cats! \n",
      "    They're playing in the beautiful gardens.'\n",
      "‚úÖ Tokens finais: ['run', 'dogs', 'good', 'than', 'cats', 'theyre', 'playing', 'beautiful', 'gardens']\n",
      "üìä Redu√ß√£o: 13 ‚Üí 9 tokens\n",
      "\n",
      "üîç COMPARA√á√ÉO: STEMMING vs LEMMATIZA√á√ÉO SIMPLES\n",
      "==================================================\n",
      "running    ‚Üí Stem: runn     | Lemma: run\n",
      "better     ‚Üí Stem: better   | Lemma: good\n",
      "playing    ‚Üí Stem: play     | Lemma: playing\n",
      "beautiful  ‚Üí Stem: beautiful | Lemma: beautiful\n",
      "quickly    ‚Üí Stem: quick    | Lemma: quickly\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "class TextPreprocessorSimplified:\n",
    "    def __init__(self, language='english'):\n",
    "        \"\"\"\n",
    "        Inicializa√ß√£o com recursos de pr√©-processamento simplificados\n",
    "        \n",
    "        Componentes principais:\n",
    "        - Stop words: lista b√°sica de palavras comuns\n",
    "        - Stemming simples: remo√ß√£o de sufixos b√°sicos\n",
    "        - Sem depend√™ncias externas\n",
    "        \"\"\"\n",
    "        # Lista b√°sica de stop words em ingl√™s\n",
    "        self.stop_words = {\n",
    "            'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',\n",
    "            'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',\n",
    "            'to', 'was', 'will', 'with', 'they', 'their', 'them', 'this',\n",
    "            'these', 'those', 'we', 'you', 'your', 'i', 'me', 'my', 'mine',\n",
    "            'our', 'ours', 'she', 'her', 'hers', 'him', 'his'\n",
    "        }\n",
    "        \n",
    "        # Regras b√°sicas de stemming (sufixos comuns)\n",
    "        self.stemming_rules = [\n",
    "            ('ing', ''),      # running ‚Üí runn\n",
    "            ('ly', ''),       # quickly ‚Üí quick\n",
    "            ('ed', ''),       # played ‚Üí play\n",
    "            ('ies', 'y'),     # flies ‚Üí fly\n",
    "            ('ied', 'y'),     # tried ‚Üí try\n",
    "            ('ies', 'y'),     # studies ‚Üí study\n",
    "            ('s', ''),        # dogs ‚Üí dog\n",
    "        ]\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        üßπ ETAPA 1: Limpeza B√°sica do Texto\n",
    "        \n",
    "        Opera√ß√µes realizadas:\n",
    "        1. Remove caracteres especiais e n√∫meros\n",
    "        2. Converte para min√∫sculas (case normalization)\n",
    "        3. Remove espa√ßos extras\n",
    "        \n",
    "        ‚úÖ Por que fazer isso?\n",
    "        - Reduz ru√≠do nos dados\n",
    "        - Padroniza formato\n",
    "        - Melhora consist√™ncia dos embeddings\n",
    "        \"\"\"\n",
    "        print(f\"   üìù Texto original: '{text}'\")\n",
    "        \n",
    "        # Remover caracteres especiais e n√∫meros\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        print(f\"   üîß Ap√≥s remo√ß√£o de especiais: '{text}'\")\n",
    "        \n",
    "        # Converter para min√∫sculas\n",
    "        text = text.lower()\n",
    "        print(f\"   üìù Ap√≥s min√∫sculas: '{text}'\")\n",
    "        \n",
    "        # Remover espa√ßos extras\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        print(f\"   ‚ú® Texto limpo: '{text}'\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        üö´ ETAPA 2: Remo√ß√£o de Stop Words\n",
    "        \n",
    "        Stop words s√£o palavras muito comuns que geralmente n√£o carregam\n",
    "        significado sem√¢ntico importante: 'the', 'and', 'is', 'in', etc.\n",
    "        \n",
    "        ‚úÖ Vantagens:\n",
    "        - Reduz dimensionalidade\n",
    "        - Foca em palavras com mais significado\n",
    "        - Melhora efici√™ncia computacional\n",
    "        \n",
    "        ‚ùå Cuidado:\n",
    "        - Pode remover contexto importante em algumas tarefas\n",
    "        - \"Not good\" ‚Üí \"good\" (perde nega√ß√£o)\n",
    "        \"\"\"\n",
    "        original_count = len(tokens)\n",
    "        filtered_tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        removed_count = original_count - len(filtered_tokens)\n",
    "        \n",
    "        print(f\"   üö´ Removidas {removed_count} stop words de {original_count} tokens\")\n",
    "        print(f\"   üìã Tokens restantes: {filtered_tokens}\")\n",
    "        \n",
    "        return filtered_tokens\n",
    "    \n",
    "    def simple_stem(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        üå± Stemming Simples com Regras B√°sicas\n",
    "        \n",
    "        Aplica regras simples de remo√ß√£o de sufixos.\n",
    "        N√£o √© t√£o preciso quanto Porter Stemmer, mas funciona sem depend√™ncias.\n",
    "        \n",
    "        Exemplos:\n",
    "        - running ‚Üí runn\n",
    "        - quickly ‚Üí quick\n",
    "        - played ‚Üí play\n",
    "        \"\"\"\n",
    "        for suffix, replacement in self.stemming_rules:\n",
    "            if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "                return word[:-len(suffix)] + replacement\n",
    "        return word\n",
    "    \n",
    "    def stem_tokens(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        üå± ETAPA 3A: Stemming Simples (Alternativa 1)\n",
    "        \n",
    "        Stemming remove sufixos para encontrar o \"radical\" da palavra.\n",
    "        Vers√£o simplificada com regras b√°sicas.\n",
    "        \n",
    "        ‚úÖ Vantagens: R√°pido, sem depend√™ncias externas\n",
    "        ‚ùå Limita√ß√µes: Menos preciso que algoritmos avan√ßados\n",
    "        \"\"\"\n",
    "        stemmed = [self.simple_stem(token) for token in tokens]\n",
    "        print(f\"   üå± Stemming simples aplicado:\")\n",
    "        for original, stemmed_word in zip(tokens, stemmed):\n",
    "            if original != stemmed_word:\n",
    "                print(f\"      {original} ‚Üí {stemmed_word}\")\n",
    "        return stemmed\n",
    "    \n",
    "    def simple_lemmatize(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        üìö Lemmatiza√ß√£o Simples com Dicion√°rio B√°sico\n",
    "        \n",
    "        Vers√£o simplificada usando um pequeno dicion√°rio de formas irregulares.\n",
    "        \"\"\"\n",
    "        # Dicion√°rio b√°sico de formas irregulares comuns\n",
    "        irregular_forms = {\n",
    "            'better': 'good',\n",
    "            'best': 'good',\n",
    "            'worse': 'bad',\n",
    "            'worst': 'bad',\n",
    "            'mice': 'mouse',\n",
    "            'children': 'child',\n",
    "            'feet': 'foot',\n",
    "            'teeth': 'tooth',\n",
    "            'men': 'man',\n",
    "            'women': 'woman',\n",
    "            'running': 'run',\n",
    "            'ran': 'run',\n",
    "            'swimming': 'swim',\n",
    "            'swam': 'swim',\n",
    "            'flying': 'fly',\n",
    "            'flew': 'fly'\n",
    "        }\n",
    "        \n",
    "        return irregular_forms.get(word, word)\n",
    "    \n",
    "    def lemmatize_tokens(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        üìö ETAPA 3B: Lemmatiza√ß√£o Simples (Alternativa 2 - Recomendada)\n",
    "        \n",
    "        Lemmatiza√ß√£o reduz palavras √† sua forma can√¥nica usando\n",
    "        um dicion√°rio b√°sico de formas irregulares.\n",
    "        \n",
    "        ‚úÖ Vantagens: Mais preciso que stemming simples\n",
    "        ‚ùå Limita√ß√µes: Dicion√°rio limitado, menos abrangente\n",
    "        \"\"\"\n",
    "        lemmatized = [self.simple_lemmatize(token) for token in tokens]\n",
    "        print(f\"   üìö Lemmatiza√ß√£o simples aplicada:\")\n",
    "        for original, lemma in zip(tokens, lemmatized):\n",
    "            if original != lemma:\n",
    "                print(f\"      {original} ‚Üí {lemma}\")\n",
    "        return lemmatized\n",
    "    \n",
    "    def preprocess_pipeline(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        üîÑ Pipeline Completo de Pr√©-processamento Simplificado\n",
    "        \n",
    "        Ordem das opera√ß√µes (importante!):\n",
    "        1. Limpeza ‚Üí 2. Tokeniza√ß√£o ‚Üí 3. Stop words ‚Üí 4. Lemmatiza√ß√£o\n",
    "        \n",
    "        üí° Dica: A ordem importa! Limpe antes de tokenizar,\n",
    "        remova stop words antes de lemmatizar.\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîÑ INICIANDO PIPELINE DE PR√â-PROCESSAMENTO SIMPLIFICADO\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Etapa 1: Limpeza\n",
    "        print(\"\\nüßπ ETAPA 1: LIMPEZA\")\n",
    "        clean_text = self.clean_text(text)\n",
    "        \n",
    "        # Etapa 2: Tokeniza√ß√£o simples\n",
    "        print(\"\\n‚úÇÔ∏è ETAPA 2: TOKENIZA√á√ÉO\")\n",
    "        tokens = clean_text.split()\n",
    "        print(f\"   üìù Tokens: {tokens}\")\n",
    "        \n",
    "        # Etapa 3: Remo√ß√£o de stop words\n",
    "        print(\"\\nüö´ ETAPA 3: REMO√á√ÉO DE STOP WORDS\")\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        \n",
    "        # Etapa 4: Lemmatiza√ß√£o simples\n",
    "        print(\"\\nüìö ETAPA 4: LEMMATIZA√á√ÉO SIMPLES\")\n",
    "        tokens = self.lemmatize_tokens(tokens)\n",
    "        \n",
    "        print(f\"\\n‚úÖ RESULTADO FINAL: {tokens}\")\n",
    "        return tokens\n",
    "\n",
    "# üöÄ EXEMPLO PR√ÅTICO EDUCACIONAL\n",
    "def demonstrar_preprocessamento_simplificado():\n",
    "    \"\"\"Demonstra√ß√£o completa do pr√©-processamento sem NLTK\"\"\"\n",
    "    \n",
    "    preprocessor = TextPreprocessorSimplified()\n",
    "    \n",
    "    # Texto com v√°rios desafios\n",
    "    texto_exemplo = \"\"\"\n",
    "    The running dogs are better than cats! \n",
    "    They're playing in the beautiful gardens.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üéì DEMONSTRA√á√ÉO: PR√â-PROCESSAMENTO DE TEXTO (VERS√ÉO SIMPLIFICADA)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Executar pipeline completo\n",
    "    resultado = preprocessor.preprocess_pipeline(texto_exemplo)\n",
    "    \n",
    "    print(\"\\nüìä RESUMO DO PROCESSAMENTO:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"üìù Texto original: '{texto_exemplo.strip()}'\")\n",
    "    print(f\"‚úÖ Tokens finais: {resultado}\")\n",
    "    print(f\"üìä Redu√ß√£o: {len(texto_exemplo.split())} ‚Üí {len(resultado)} tokens\")\n",
    "    \n",
    "    # Comparar stemming vs lemmatiza√ß√£o simples\n",
    "    print(\"\\nüîç COMPARA√á√ÉO: STEMMING vs LEMMATIZA√á√ÉO SIMPLES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    tokens_exemplo = ['running', 'better', 'playing', 'beautiful', 'quickly']\n",
    "    \n",
    "    for token in tokens_exemplo:\n",
    "        stemmed = preprocessor.simple_stem(token)\n",
    "        lemmatized = preprocessor.simple_lemmatize(token)\n",
    "        print(f\"{token:10} ‚Üí Stem: {stemmed:8} | Lemma: {lemmatized}\")\n",
    "\n",
    "# Executar demonstra√ß√£o\n",
    "demonstrar_preprocessamento_simplificado()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc9bef",
   "metadata": {},
   "source": [
    "### üéØ **Pontos-Chave para Fixa√ß√£o**\n",
    "\n",
    "1. **Tokeniza√ß√£o √© fundamental**: √â o primeiro passo para converter texto em dados process√°veis\n",
    "2. **Diferentes m√©todos, diferentes prop√≥sitos**: Simples para prototipagem, NLTK para an√°lise geral, Transformers para modelos modernos\n",
    "3. **Pr√©-processamento melhora qualidade**: Texto limpo gera embeddings mais consistentes\n",
    "4. **Ordem importa**: Sempre siga a sequ√™ncia l√≥gica de processamento\n",
    "5. **Trade-offs**: Mais processamento = mais lento, mas geralmente melhor qualidade\n",
    "\n",
    "### üí° **Dicas Pr√°ticas**\n",
    "\n",
    "- **Para embeddings**: Use tokeniza√ß√£o compat√≠vel com o modelo escolhido\n",
    "- **Para an√°lise explorat√≥ria**: NLTK √© uma boa escolha\n",
    "- **Para produ√ß√£o**: Considere performance vs. qualidade\n",
    "- **Sempre valide**: Inspecione os resultados de cada etapa"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
